{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastT5 import export_and_get_onnx_model, OnnxT5, get_onnx_runtime_sessions, get_onnx_model\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exporting to onnx... |################################| 3/3\n",
      "Quantizing... |################################| 3/3\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up onnx model...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "base_model_path = '/home/falk/summarization/outputs/t5-large-paraphrased_twitter/t5-large-paraphrased_twitter(1)/best_model'\n",
    "# model_or_model_path = '/home/falk/paraphrasing/outputs/t5-large/t5-large-paraphrasing/best_model'\n",
    "model = export_and_get_onnx_model(base_model_path, input_sequence_length=1000)\n",
    "# get_onnx_runtime_sessions(\"models\")\n",
    "# model = OnnxT5(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-17 14:07:00.061248270 [W:onnxruntime:, inference_session.cc:506 RegisterExecutionProvider] Parallel execution mode does not support the CUDA Execution Provider. So making the execution mode sequential for this session since it uses the CUDA Execution Provider.\n",
      "2022-01-17 14:07:01.954480773 [W:onnxruntime:, inference_session.cc:506 RegisterExecutionProvider] Parallel execution mode does not support the CUDA Execution Provider. So making the execution mode sequential for this session since it uses the CUDA Execution Provider.\n",
      "2022-01-17 14:07:02.952320974 [W:onnxruntime:, inference_session.cc:506 RegisterExecutionProvider] Parallel execution mode does not support the CUDA Execution Provider. So making the execution mode sequential for this session since it uses the CUDA Execution Provider.\n"
     ]
    }
   ],
   "source": [
    "t5_large_paths = \"models/best_model-encoder.onnx\", \"models/best_model-decoder.onnx\", \"models/best_model-init-decoder.onnx\"\n",
    "model = OnnxT5(\"t5-large\", get_onnx_runtime_sessions(t5_large_paths, default=False))\n",
    "# model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"/home/falk/summarization/datasets/paraphrased_twitter\")\n",
    "input_prepend = \"Summarize: \"\n",
    "inputs = dataset['test'][2][\"article\"]\n",
    "text = inputs\n",
    "token = tokenizer(text, padding=\"max_length\", max_length=1024, return_tensors='pt')\n",
    "# token = token.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.generate(input_ids=token['input_ids'],\n",
    "               attention_mask=token['attention_mask'],\n",
    "               num_beams=1,\n",
    "               max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The winter storm brings wind, rain, snow to the US west A powerful storm packed heavy rain, snow and wind pounded parts of California and western Nevada early Wednesday. The Washington Post reported that it could be the largest snowfall the area has seen in two years. Geider's suggestion to those expecting to travel is to take extra time and be prepared. The storm will bring the heaviest snow to eastern Iowa, southern Wisconsin, northern Illinois, and parts of Indiana and Ohio, The Weather Channel reported. ABC reports that Chicago may see up to ten inches of snow. The heavy precipitation was a result of an atmospheric river, which are thousands of miles long ribbons of water vapor, that strengthen rain and snow in the West Coast.\"]\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1024]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = token['attention_mask'].shape\n",
    "list(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(10)\n",
    "# x = x.to('cuda')\n",
    "print(x.device)\n",
    "print(x.device.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "X = np.ones(10)\n",
    "x_ortvalue = onnxruntime.OrtValue.ortvalue_from_numpy(X, 'cuda', 0)\n",
    "X.dtype\n",
    "np.longlong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ortvalue.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[3111, 4000, 1575,  ...,    0,    0,    0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pkv_0', 10), ('pkv_1', 20), ('pkv_2', 30), ('pkv_0', 10)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flat_past_key_values = [10, 20, 30]\n",
    "past_key_values = [\n",
    "    (f\"pkv_{i}\", pkv) for i, pkv in enumerate(flat_past_key_values)\n",
    "]\n",
    "\n",
    "past_key_values.append(list((f\"pkv_{i}\", pkv) for i, pkv in enumerate(flat_past_key_values))[0])\n",
    "past_key_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('fastt5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e21c61e8d02188f4180a4b5468d076205d2b2fae2a38d16b9e98f6eb268d1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
